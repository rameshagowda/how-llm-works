═══════════════════════════════════════════════════════════════════════════════
                           LLM BRAIN MAP — VISUAL GUIDE
═══════════════════════════════════════════════════════════════════════════════

INTRODUCTION
────────────────────────────────────────────────────────────────────────────────
Large Language Models (LLMs) convert user text into meaning, reason over it, 
and generate responses one token at a time. This manual explains the full 
pipeline using visuals, one-liner definitions, examples, and analogies.

═══════════════════════════════════════════════════════════════════════════════
                        SECTION 1 — FULL PIPELINE
═══════════════════════════════════════════════════════════════════════════════

User Input: "How does an LLM work?"

┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 1 — TOKENIZATION                                                    │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Breaks text into tokens                                      │
│ Example:    "LLM" → [" L", "LM"]                                        │
│ Analogy:    Cutting a sentence into Lego pieces                         │
│                                                                           │
│ Visual:     [How] [ does] [ an] [ LLM] [ work] [?]                      │
└───────────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 2 — EMBEDDINGS                                                      │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Converts tokens into vectors representing meaning            │
│ Example:    "LLM" → [0.12, -0.44, 0.88]                                 │
│ Analogy:    Assigning each Lego piece a unique color pattern            │
│                                                                           │
│ Visual:     Token → Meaning Vector                                       │
└───────────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 3 — POSITIONAL ENCODING                                             │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Adds order so the model knows sequence                       │
│ Example:    "How" is 1st, "LLM" is 4th                                  │
│ Analogy:    Page numbers in a book                                       │
│                                                                           │
│ Visual:     Token Vector + Position Vector = Ordered Meaning Vector      │
└───────────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 4 — TRANSFORMER LAYERS (THE REASONING ENGINE)                       │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Core engine that understands relationships                   │
│                                                                           │
│ ┌─────────────────────────────────────────────────────────────────────┐ │
│ │ 4A — SELF-ATTENTION                                                 │ │
│ ├─────────────────────────────────────────────────────────────────────┤ │
│ │ One-liner: Determines relevance between tokens                      │ │
│ │ Example:   "LLM" attends to "work"                                 │ │
│ │ Analogy:   Highlighting important words while reading               │ │
│ │                                                                     │ │
│ │ Visual:    "LLM" ←──────→ "work"                                   │ │
│ │            "How" ←──────→ "does"                                   │ │
│ └─────────────────────────────────────────────────────────────────────┘ │
│                                                                           │
│ ┌─────────────────────────────────────────────────────────────────────┐ │
│ │ 4B — MULTI-HEAD ATTENTION                                           │ │
│ ├─────────────────────────────────────────────────────────────────────┤ │
│ │ One-liner: Looks at input from multiple perspectives               │ │
│ │ Example:   One head tracks grammar, another tracks meaning          │ │
│ │ Analogy:   A team of experts analyzing the same sentence            │ │
│ │                                                                     │ │
│ │ Heads:                                                              │ │
│ │   • Head 1 → Grammar links        ("does" ↔ "work")               │ │
│ │   • Head 2 → Semantic meaning     ("LLM" ↔ "model")               │ │
│ │   • Head 3 → Long-range links     ("How" ↔ "work")                │ │
│ │   • Head 4 → Question focus       (emphasizes "How")               │ │
│ │   • Head 5 → Topic detection      (identifies "LLM")               │ │
│ │   • Head 6 → Answer structure     (predicts explanation)           │ │
│ └─────────────────────────────────────────────────────────────────────┘ │
│                                                                           │
│ ┌─────────────────────────────────────────────────────────────────────┐ │
│ │ 4C — FEED-FORWARD NETWORKS                                          │ │
│ ├─────────────────────────────────────────────────────────────────────┤ │
│ │ One-liner: Refines meaning after attention                          │ │
│ │ Example:   Cleans up noisy signals                                  │ │
│ │ Analogy:   Polishing a rough diamond                                │ │
│ │                                                                     │ │
│ │ Visual:    [Attention Output] → [Refinement Layer]                 │ │
│ └─────────────────────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 5 — FINAL HIDDEN STATE                                              │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Model's internal understanding of your input                 │
│ Example:    A compressed meaning of your entire question                 │
│ Analogy:    A summary in the model's mind                                │
│                                                                           │
│ Visual:     [Deep Meaning Vector]                                        │
└───────────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 6 — OUTPUT PROBABILITIES                                            │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Predicts likelihood of each next token                       │
│ Example:    "A large" → 74%                                              │
│             "An"      → 5%                                               │
│ Analogy:    Choosing the most likely next word                           │
│                                                                           │
│ Visual:     Token → Probability Bar                                      │
└───────────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ STEP 7 — DECODING LOOP                                                   │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Generates answer token-by-token                              │
│ Example:    "A" → "A large" → "A large language model..."               │
│ Analogy:    Writing a sentence one word at a time                        │
│                                                                           │
│ Visual:     Step 1: A                                                    │
│             Step 2: A large                                              │
│             Step 3: A large language                                     │
│             Step 4: A large language model                               │
└───────────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════
                    SECTION 2 — DECODING STRATEGIES
═══════════════════════════════════════════════════════════════════════════════

┌───────────────────────────────────────────────────────────────────────────┐
│ GREEDY DECODING                                                           │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Always pick highest probability token                        │
│ Example:    Predictable, safe output                                     │
│ Analogy:    Always choosing the safest option                            │
└───────────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ TOP-K SAMPLING                                                            │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Pick from top k tokens                                       │
│ Example:    k=5 → more variety in output                                 │
│ Analogy:    Choosing from top 5 menu items                               │
└───────────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ TOP-P (NUCLEUS) SAMPLING                                                  │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Pick from tokens covering p% probability                     │
│ Example:    p=0.9 → dynamic flexibility                                  │
│ Analogy:    Choose from items that make up 90% of popularity             │
└───────────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────────┐
│ TEMPERATURE                                                               │
├───────────────────────────────────────────────────────────────────────────┤
│ Definition: Controls randomness in token selection                       │
│ Example:    High temp → creative, Low temp → focused                     │
│ Analogy:    Turning up the "creativity thermostat"                       │
└───────────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════
                    SECTION 3 — COMPLETE PIPELINE FLOW
═══════════════════════════════════════════════════════════════════════════════

                            [Input Text]
                                 ↓
                         ┌───────────────┐
                         │ TOKENIZATION  │
                         └───────┬───────┘
                                 ↓
                         ┌───────────────┐
                         │  EMBEDDINGS   │
                         └───────┬───────┘
                                 ↓
                         ┌───────────────┐
                         │  POSITIONAL   │
                         │   ENCODING    │
                         └───────┬───────┘
                                 ↓
                    ┌────────────────────────┐
                    │   TRANSFORMER LAYERS   │
                    ├────────────────────────┤
                    │  • Self-Attention      │
                    │  • Multi-Head Attn     │
                    │  • Feed-Forward        │
                    └────────────┬───────────┘
                                 ↓
                         ┌───────────────┐
                         │ FINAL HIDDEN  │
                         │     STATE     │
                         └───────┬───────┘
                                 ↓
                         ┌───────────────┐
                         │    OUTPUT     │
                         │ PROBABILITIES │
                         └───────┬───────┘
                                 ↓
                         ┌───────────────┐
                         │  DECODING     │
                         │     LOOP      │
                         └───────┬───────┘
                                 ↓
                          [Final Output]

═══════════════════════════════════════════════════════════════════════════════
                        SECTION 4 — BRAIN MAP VISUAL
═══════════════════════════════════════════════════════════════════════════════

                      "How does an LLM work?"
                               │
                               ▼
                    ╔═══════════════════╗
                    ║  TOKENIZATION     ║
                    ╠═══════════════════╣
                    ║ Breaks text into  ║
                    ║ tokens            ║
                    ╚═════════╦═════════╝
                              │
          Example: "LLM" → [" L", "LM"]
          Analogy: Cutting sentence into Lego pieces
                              │
                              ▼
                    ╔═══════════════════╗
                    ║   EMBEDDINGS      ║
                    ╠═══════════════════╣
                    ║ Turns tokens into ║
                    ║ meaning vectors   ║
                    ╚═════════╦═════════╝
                              │
          Example: "LLM" → [0.12, -0.44, 0.88]
          Analogy: Assigning each piece a color pattern
                              │
                              ▼
                    ╔═══════════════════╗
                    ║  POSITIONAL       ║
                    ║  ENCODING         ║
                    ╠═══════════════════╣
                    ║ Adds sequence     ║
                    ║ order info        ║
                    ╚═════════╦═════════╝
                              │
          Example: "How" is 1st, "LLM" is 4th
          Analogy: Page numbers in a book
                              │
                              ▼
              ╔═══════════════════════════════╗
              ║   TRANSFORMER LAYERS          ║
              ║   (Core Reasoning Engine)     ║
              ╠═══════════════════════════════╣
              ║                               ║
              ║  ┌─────────────────────────┐  ║
              ║  │ SELF-ATTENTION          │  ║
              ║  │ Determines relevance    │  ║
              ║  └─────────────────────────┘  ║
              ║  Example: "LLM" ↔ "work"      ║
              ║  Analogy: Highlighting words  ║
              ║                               ║
              ║  ┌─────────────────────────┐  ║
              ║  │ MULTI-HEAD ATTENTION    │  ║
              ║  │ Multiple perspectives   │  ║
              ║  └─────────────────────────┘  ║
              ║  Example: Grammar + Meaning   ║
              ║  Analogy: Team of experts     ║
              ║                               ║
              ║  ┌─────────────────────────┐  ║
              ║  │ FEED-FORWARD NETWORKS   │  ║
              ║  │ Refines meaning         │  ║
              ║  └─────────────────────────┘  ║
              ║  Example: Cleans signals      ║
              ║  Analogy: Polishing diamond   ║
              ║                               ║
              ╚═══════════════╦═══════════════╝
                              │
                              ▼
                    ╔═══════════════════╗
                    ║ FINAL HIDDEN      ║
                    ║ STATE             ║
                    ╠═══════════════════╣
                    ║ Internal          ║
                    ║ understanding     ║
                    ╚═════════╦═════════╝
                              │
          Example: Compressed meaning of question
          Analogy: Summary in model's mind
                              │
                              ▼
                    ╔═══════════════════╗
                    ║ OUTPUT            ║
                    ║ PROBABILITIES     ║
                    ╠═══════════════════╣
                    ║ Predicts next     ║
                    ║ token likelihood  ║
                    ╚═════════╦═════════╝
                              │
          Example: "A large" = 74% probability
          Analogy: Choosing likely next word
                              │
                              ▼
                    ╔═══════════════════╗
                    ║ DECODING LOOP     ║
                    ╠═══════════════════╣
                    ║ Generates answer  ║
                    ║ token-by-token    ║
                    ╚═════════╦═════════╝
                              │
          Example: "A" → "A large" → "A large language model..."
          Analogy: Writing sentence one word at a time
                              │
                              ▼
                "A large language model..."

═══════════════════════════════════════════════════════════════════════════════
                                END OF GUIDE
═══════════════════════════════════════════════════════════════════════════════
